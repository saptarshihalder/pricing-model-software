# Dzukou Dynamic Pricing Toolkit

This project provides a lightweight workflow for analysing competitor prices and recommending new pricing for Dzukou products. The repository contains small utilities for scraping online stores, running price optimisation algorithms and visualising the expected impact on profit.

## Repository overview

- `manage_products.py` – GUI tool to register products and category keywords. It updates the overview CSV and maintains a mapping between products and their competitor data files.
- `scraper.py` – searches a set of sustainable shops for products related to each category and saves the prices in CSV files inside the `product_data/` directory.
- `price_optimizer.py` – processes the scraped prices together with the current price and unit cost of each product to suggest a new selling price.
- `dashboard.py` – builds an interactive HTML dashboard showing the recommended prices, expected profit changes and basic A/B test metrics.
- Data files:
  - `Dzukou_Pricing_Overview_With_Names - Copy.csv` – summary of the product catalogue with current prices and unit costs.
  - `product_data_mapping.csv` – links product names to the CSV file where competitor prices are stored.
  - `category_keywords.json` – extra keywords used by the scraper to search for each category.
  - `product_data/` – per-category CSV files generated by the scraper.
  - `recommended_prices.csv` – result of the optimiser.

## Installation

1. Install Python 3.10 or newer.
2. Install the required Python packages:

```bash
pip install pandas plotly requests scikit-optimize selenium beautifulsoup4
```

`scikit-optimize` is optional. If it is not installed the optimiser falls back to a grid search strategy.


## Tutorial

1. **Add your products**
   
   Run:
   ```bash
   python manage_products.py
   ```
   This opens a small window where you can enter the product name, an internal ID, its category, current selling price and unit cost. Provide comma separated keywords that describe the category. The tool creates an entry in the overview CSV and generates (or updates) a CSV file under `product_data/` for competitor prices. New categories are automatically inserted into `scraper.py` and `category_keywords.json` so future scrapes include them.

2. **Scrape competitor prices**
   
   Once products and categories are set up, run the scraper:
   ```bash
   python scraper.py
   ```
   The script searches a handful of sustainable online stores for each configured search term. Results are saved to CSV files inside `product_data/`. The output lists how many products were found for each category and store.

3. **Generate price recommendations**
   
   After collecting competitor data run the optimiser:
   ```bash
   python price_optimizer.py
   ```
   For each product the script reads the relevant CSV file, cleans the scraped prices and applies a profit model. It uses Bayesian optimisation (from `scikit-optimize`) when available, otherwise a grid search, to maximise expected profit while respecting category-specific constraints. The output CSV `recommended_prices.csv` contains statistics such as the competitor price range, the suggested price and the estimated profit delta. A simple A/B test simulator provides a p‑value to gauge if the change is statistically significant.

4. **Visualise the results**
   
   Create an HTML dashboard with:
   ```bash
   python dashboard.py
   ```
   This generates `dashboard.html` showing bar charts of profit impact, current vs recommended prices, price change percentages and significance levels. Open the HTML file in a browser to explore the results interactively.

## How it works

1. **Scraping** – `scraper.py` can use `requests` for simple pages or fall back to Selenium for sites that require JavaScript rendering. The scraper stores the product name, price, originating store and search term. Duplicate entries are removed and each category has its own CSV file.
2. **Price optimisation** – `price_optimizer.py` reads the competitor prices, current price and unit cost. A logistic demand model estimates profit across different price points. The search respects bounds based on competitor prices and configurable limits on price increases or decreases. Category specific parameters (margins, demand elasticity, etc.) can be tuned in the script.
3. **A/B simulation** – the optimiser includes a function `run_ab_test` which simulates a simple control/test experiment with stochastic demand. The resulting profit difference and p‑value are written to the recommendation file.
4. **Dashboard** – the dashboard script merges the recommendations with the overview data to compute price deltas. It then renders interactive Plotly charts inside a styled HTML template.

## Example workflow

```bash
# 1. Register your products
python manage_products.py

# 2. Scrape competitor websites (may take some time)
python scraper.py

# 3. Compute new prices and evaluate profit impact
python price_optimizer.py

# 4. Review the results visually
python dashboard.py
```

The generated CSV and HTML files can be shared with the rest of the team for review. Feel free to customise the scripts to add stores, adjust the demand model or extend the dashboard.

